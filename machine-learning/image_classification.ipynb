{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4u4YC8iqxpe"
      },
      "source": [
        "# CODEATHON 2: Recognizing UVA landmarks with neural nets (50 pts)\n",
        "![UVA Grounds](https://giving.virginia.edu/sites/default/files/2019-02/jgi-teaser-image.jpg)\n",
        "\n",
        "The UVA Grounds is known for its Jeffersonian architecture and place in U.S. history as a model for college and university campuses throughout the country. Throughout its history, the University of Virginia has won praises for its unique Jeffersonian architecture.\n",
        "\n",
        "In this codeathon, you will attempt the build an image recognition system to classify different buildlings/landmarks on Grounds. You will earn 50 points for this codeathon plus 10 bonus points. To make it easier for you, some codes have been provided to help you process the data, you may modify it to fit your needs. You must submit the .ipynb file via UVA Collab with the following format: yourcomputingID_codeathon_2.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIAunzfaCNAH"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import sklearn\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whpECRG_B2Nj"
      },
      "source": [
        "# Step 2: Process the  Dataset\n",
        "The full dataset is huge (+37GB) with +13K images of 18 classes. So it will take a while to download, extract, and process. To save you time and effort, a subset of the data has been resized and compressed to only 379Mb and stored in my Firebase server. This dataset will be the one you will benchmark for your grade. If you are up for a challenge (and perhaps bonus points), contact the instructor for the full dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4E5kiDN9OFs"
      },
      "outputs": [],
      "source": [
        "# Download dataset from Firebase\n",
        "!wget https://firebasestorage.googleapis.com/v0/b/uva-landmark-images.appspot.com/o/dataset.zip?alt=media&token=e1403951-30d6-42b8-ba4e-394af1a2ddb7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKWszcAd9WJh"
      },
      "outputs": [],
      "source": [
        "# Extract content\n",
        "!unzip \"/content/dataset.zip?alt=media\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "474nVh3m-FrM"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_files\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from tqdm import tqdm # progress bar\n",
        "\n",
        "data_dir = \"/content/dataset/\"\n",
        "batch_size = 32;\n",
        "# IMPORTANT: Depends on what pre-trained model you choose, you will need to change these dimensions accordingly\n",
        "img_height = 150;\n",
        "img_width = 150;\n",
        "\n",
        "# Training Dataset\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split = 0.2,\n",
        "    subset = \"training\",\n",
        "    seed = 42,\n",
        "    image_size= (img_height, img_width),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "# Validation Dataset\n",
        "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split = 0.2,\n",
        "    subset = \"validation\",\n",
        "    seed = 42,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gWDUhbDDFZn"
      },
      "outputs": [],
      "source": [
        "# Visualize some of the train samples of one batch\n",
        "# Make sure you create the class names that match the order of their appearances in the \"files\" variable\n",
        "class_names = ['AcademicalVillage', 'AldermanLibrary', 'AlumniHall', 'AquaticFitnessCenter',\n",
        "  'BavaroHall', 'BrooksHall', 'ClarkHall', 'MadisonHall', 'MinorHall', 'NewCabellHall',\n",
        "  'NewcombHall', 'OldCabellHall', 'OlssonHall', 'RiceHall', 'Rotunda', 'ScottStadium',\n",
        "  'ThorntonHall', 'UniversityChapel']\n",
        "\n",
        "# Rows and columns are set to fit one training batch (32)\n",
        "n_rows = 8\n",
        "n_cols = 4\n",
        "plt.figure(figsize=(n_cols * 3, n_rows * 3))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range (n_rows*n_cols):\n",
        "        plt.subplot(n_rows, n_cols, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[labels[i]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=.2, hspace=.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP0-jzrMYioK"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "print(len(class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L5V5__CnzWP"
      },
      "source": [
        "# Step 3: Create your own CNN architecture\n",
        "You must design your own architecture. To get started, you may get inspiration from one in CNN notebook  (i.e. use one similar to LeNet-5 or AlexNet). You will have to report the design of the architecture:\n",
        "\n",
        "1.   How many layers does it have?\n",
        "2.   Why do you decide on a certain number nodes per layer?\n",
        "3.   Which activation functions do you choose?\n",
        "4.   How many parameters does it has in total?\n",
        "\n",
        "Hint: use `myModel.summary()` to learn on the layers and parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_BASiJqrvMa"
      },
      "source": [
        "# My Model Starts Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY39CrRHe3V9"
      },
      "source": [
        "The following architecture is based on the ALEXNET model. I have changed the input shape. When playing with my own models I could not achieve an validation accuracy above 0.1071\n",
        "source: https://thecleverprogrammer.com/2021/12/13/alexnet-architecture-using-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS6GVd2biq14"
      },
      "outputs": [],
      "source": [
        "!pip install visualkeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZfEgQIUnzWY"
      },
      "outputs": [],
      "source": [
        "import visualkeras\n",
        "\n",
        "myModel = keras.models.Sequential()\n",
        "\n",
        "myModel.add(keras.layers.Conv2D(filters=96, kernel_size=(11, 11),\n",
        "                        strides=(4, 4), activation=\"relu\",\n",
        "                        input_shape=(150, 150, 3)))\n",
        "myModel.add(keras.layers.BatchNormalization())\n",
        "myModel.add(keras.layers.MaxPool2D(pool_size=(3, 3), strides= (2, 2)))\n",
        "myModel.add(keras.layers.Conv2D(filters=256, kernel_size=(5, 5),\n",
        "                        strides=(1, 1), activation=\"relu\",\n",
        "                        padding=\"same\"))\n",
        "myModel.add(keras.layers.BatchNormalization())\n",
        "myModel.add(keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "myModel.add(keras.layers.Conv2D(filters=384, kernel_size=(3, 3),\n",
        "                        strides=(1, 1), activation=\"relu\",\n",
        "                        padding=\"same\"))\n",
        "myModel.add(keras.layers.BatchNormalization())\n",
        "myModel.add(keras.layers.Conv2D(filters=384, kernel_size=(3, 3),\n",
        "                        strides=(1, 1), activation=\"relu\",\n",
        "                        padding=\"same\"))\n",
        "myModel.add(keras.layers.BatchNormalization())\n",
        "myModel.add(keras.layers.Conv2D(filters=256, kernel_size=(3, 3),\n",
        "                        strides=(1, 1), activation=\"relu\",\n",
        "                        padding=\"same\"))\n",
        "myModel.add(keras.layers.BatchNormalization())\n",
        "myModel.add(keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "myModel.add(keras.layers.Flatten())\n",
        "myModel.add(keras.layers.Dense(4096, activation=\"relu\"))\n",
        "myModel.add(keras.layers.Dropout(0.5))\n",
        "myModel.add(keras.layers.Dense(18, activation=\"softmax\"))\n",
        "\n",
        "myModel.summary()\n",
        "visualkeras.layered_view(myModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33s9AMe6ok9x"
      },
      "source": [
        "After designing the model, you will need to train it. In order to train, you will need to pick a number of `epoch` (iteration), which `optimizer` to use (from  `keras.optimizers`), a `loss` function, and some `metrics`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYsKzVh0pKRb"
      },
      "outputs": [],
      "source": [
        "Epochs = 10 ##TODO\n",
        "Optimizer = keras.optimizers.Adam(learning_rate=1e-3) #TODO\n",
        "Loss = \"sparse_categorical_crossentropy\" #TODO\n",
        "Metrics = \"accuracy\" #TODO keep in mind that this can be multiple metrics including at least the accuracy\n",
        "myModel.compile(loss= Loss, optimizer = Optimizer, metrics = Metrics)\n",
        "myHistory = myModel.fit(train_ds,\n",
        "                      validation_data=validation_ds,\n",
        "                      epochs = Epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDLqsjlkoNay"
      },
      "source": [
        "Next, you need to create (1) a plot of training and validation `loss` and (2) a plot of training and validation `accuracy`. These plots might give you some insights about your model performance and possibility of overfitting.\n",
        "\n",
        "Report the performance of your architecture on the validation set in a `confusion matrix`. Make comments on the performance by answering the following questiosns:\n",
        "- How well do you think your architecture is doing (overall accuracy)?\n",
        "- Where did it makes mistake most?\n",
        "- Which classes can be improved?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dot7SiaQA3Bk"
      },
      "outputs": [],
      "source": [
        "# From slides\n",
        "# def plot_learning_curves(loss, accuracy):\n",
        "#     plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"Training loss\")\n",
        "#     plt.plot(np.arange(len(accuracy)) + 1, accuracy, \"r.-\", label=\"Accuracy\")\n",
        "#     plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
        "#     plt.axis([1, 10, -0.02, 3])\n",
        "#     plt.legend(fontsize=14)\n",
        "#     plt.xlabel(\"Epochs\")\n",
        "#     plt.ylabel(\"Loss\")\n",
        "#     plt.grid(True)\n",
        "\n",
        "def plot_loss_curves(train, val):\n",
        "    plt.plot(np.arange(len(train)) + 0.5, train, \"b.-\", label=\"Training loss\")\n",
        "    plt.plot(np.arange(len(val)) + 1, val, \"r.-\", label=\"Validation loss\")\n",
        "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
        "    # plt.axis([1, 10, -0.02, 3])\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "\n",
        "def plot_accuracy_curves(train, val):\n",
        "    plt.plot(np.arange(len(train)) + 0.5, train, \"b.-\", label=\"Training accuracy\")\n",
        "    plt.plot(np.arange(len(val)) + 1, val, \"r.-\", label=\"Validation accuracy\")\n",
        "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
        "    # plt.axis([1, 10, -0.02, 3])\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FhpZM_ErHqn"
      },
      "outputs": [],
      "source": [
        "# Your evaluation code here\n",
        "plot_loss_curves(myHistory.history[\"loss\"], myHistory.history[\"val_loss\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_accuracy_curves(myHistory.history[\"accuracy\"], myHistory.history[\"val_accuracy\"])"
      ],
      "metadata": {
        "id": "9VnNfboDaW4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1mqyjI7FTYk"
      },
      "outputs": [],
      "source": [
        "pics = []\n",
        "actual_values = []\n",
        "\n",
        "for images, labels in validation_ds:\n",
        "  for i in range(images.shape[0]):\n",
        "    pics.append(images[i].numpy().astype(\"uint8\"))\n",
        "    actual_values.append(labels[i].numpy())\n",
        "\n",
        "val_labels = np.asarray(actual_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrFr-vgHBlrZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "expected = actual_values\n",
        "y_pred = myModel.predict(validation_ds)\n",
        "# print(len(expected), len(y_pred))\n",
        "# print(expected[0:5], \"\\n\\n\", y_pred[0:5])\n",
        "lst = []\n",
        "for i in range(len(y_pred)):\n",
        "  y_pred[i] = np.argmax(y_pred[i])\n",
        "  lst.append(y_pred[i][0])\n",
        "\n",
        "pred_labels = np.asarray(lst)\n",
        "\n",
        "print(type(pred_labels))\n",
        "print(type(val_labels))\n",
        "print(pred_labels[0:5], \"\\n\\n\", val_labels[0:5])\n",
        "# print(y_pred[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gcgbf7-JPWh"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = sklearn.metrics.confusion_matrix(pred_labels, val_labels)\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIOaqU5nQ_B_"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "con_mat_df = pd.DataFrame(confusion_matrix,\n",
        "                     index = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
        "                     columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17])\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report Model Configuration\n",
        "**Num layers:** 17\n",
        "\n",
        "**Justification for Nodes per layer:** This is the node density that was proposed in the orginial AlexNet. The pattern is to apply increasing kernel layers and decreasing kernel sizes which makes sense for examining big picture then small details of an image. The large number of filters is important for examining many different \"angles\" or prospectives on an image. Also the final hidden layer is very dense (4096 fully connected) with no dropout so their is a potenitel for overfitting which is observed in the graphs as training always preforms substaintaly better than validation\n",
        "\n",
        "**Activation Functions:** ReLu becuase it is the fastest activation function for training models (~6x faster than sigmoid and tanh)\n",
        "\n",
        "**Total Number of Free Parameters:**\n",
        "\n",
        "Total params: 13,267,730\n",
        "\n",
        "Trainable params: 13,264,978\n",
        "\n",
        "Non-trainable params: 2,752\n",
        "\n",
        "## Analysis of Model\n",
        "**How well is my architecture is doing (overall accuracy):** Validation Accuracy of 75%\n",
        "\n",
        "**Where did my model makes mistake most?** Label 13 was the worse preformence as identified by scanning incrorrect predictions across a confusion matrix row comparing ratio of mislabeled : correct labeled. Label 14 was comparibly bad as well. 'OlssonHall' and 'RiceHall' respectively.\n",
        "\n",
        "**Which classes can be improved?** Most of them could be improved. From the density of actual labels I do not think the image set is balenced which makes the NN predict the more frequent images more than less likely images. There is not a clear \"accurate\" diagonal in this confusion matrix but is stronger in existence for lower number labels. Again I think this is becuase proportionally their are more images of the lower labels."
      ],
      "metadata": {
        "id": "V1NMb5sUtVLB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJF5CTcOl2Fy"
      },
      "source": [
        "# Step 4: Use a Pre-trained Network with Transfer Learning\n",
        "Now that you have a your own custom model and some baseline performance, let's see if you can improve the performance using transfer learning and a pre-trained model. You may use any pre-trained model EXCEPT ones that already provided such as `Xception`, `MobileNet`, `EfficientNetB6`. Keep in mind that each pre-trained model may expect a different input shape, so adjust the size of your training images accordingly.\n",
        "\n",
        "Make sure you report the design of this architecture by answer the same questions 1-4 in Step 3.\n",
        "\n",
        "Hint: use `ImageNet` as weights when load the pre-train network, then add a `GlobalAveragePooling2D` and an output layer with `softmax` activation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwqq_K0MOsqF"
      },
      "outputs": [],
      "source": [
        "!pip install keras_applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJlRl1i-l970"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "from tensorflow.keras.applications import ResNet50, resnet50\n",
        "base_model = ResNet50(weights='imagenet', include_top=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28_pJhz5Yrvl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python import train\n",
        "# https://stackoverflow.com/questions/70091290/tensorflow-datasets-crop-resize-images-per-batch-after-dataset-batch\n",
        "# def resize_data(images):\n",
        "#   tf.print('Original shape -->', tf.shape(images))\n",
        "#   SIZE = (224, 224)\n",
        "\n",
        "#   return tf.image.resize_with_crop_or_pad(images, SIZE[0], SIZE[1])\n",
        "\n",
        "def preprocess(image, label):\n",
        "  resized = keras.applications.resnet50.preprocess_input(image)\n",
        "  return resized, label\n",
        "\n",
        "train_images_resized = train_ds.map(preprocess)\n",
        "validation_images_resized = validation_ds.map(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_13nh6H9uXWm"
      },
      "source": [
        "Next, you will attempt to adapt this pre-trained model to your UVA Landmark dataset. It is recommended that you tried the two-phase training approach for your model:\n",
        "\n",
        "1.   Phase 1: Freeze the pre-train weights and only train the top layer\n",
        "2.   Phase 2: Train the entire network with much smaller learning rate (adapt the model to UVA data, but avoid destroying the transfered weights).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pChHvQqVvfPL"
      },
      "outputs": [],
      "source": [
        "# Phase 1 code here\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# How to do this source: https://towardsdatascience.com/build-a-custom-resnetv2-with-the-desired-depth-92892ec79d4b\n",
        " # Add classifier on top.\n",
        "# v2 has BN-ReLU before Pooling\n",
        "# X = BatchNormalization()(X)\n",
        "# X = Activation('relu')(X)\n",
        "# X = AveragePooling2D(pool_size=8)(X)\n",
        "# y = Flatten()(X)\n",
        "# y = Dense(512, activation='relu')(y)\n",
        "# y = BatchNormalization()(y)\n",
        "# y = Dropout(0.5)(y)\n",
        "\n",
        "# outputs = Dense(num_classes,\n",
        "#                 activation='softmax')(y)\n",
        "\n",
        "# # Instantiate model.\n",
        "# model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "#one hidden layers\n",
        "hidden = keras.layers.Dense(1024, activation='relu')(avg)\n",
        "red = keras.layers.Dropout(0.5)(hidden)\n",
        "#output layers\n",
        "output = keras.layers.Dense(18, activation=\"softmax\")(red)\n",
        "\n",
        "franken_model = keras.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = True\n",
        "\n",
        "franken_model.summary()\n",
        "print(len(franken_model.layers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swnOoePTvhyH"
      },
      "outputs": [],
      "source": [
        "# Phase 2 code here\n",
        "frankenOptimizer = keras.optimizers.Adamax()\n",
        "\n",
        "frankenLoss = \"sparse_categorical_crossentropy\"\n",
        "frankenMetrics = \"accuracy\"\n",
        "\n",
        "franken_model.compile(frankenOptimizer, frankenLoss, metrics = frankenMetrics)\n",
        "frankenHistory = franken_model.fit(train_images_resized,\n",
        "                      validation_data=validation_images_resized,\n",
        "                      epochs = 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(frankenHistory.history[\"loss\"], frankenHistory.history[\"val_loss\"])"
      ],
      "metadata": {
        "id": "5stD76yobS5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_accuracy_curves(frankenHistory.history[\"accuracy\"], frankenHistory.history[\"val_accuracy\"])"
      ],
      "metadata": {
        "id": "DTMhdm9ybDor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0qwDhE9uHW2"
      },
      "source": [
        "Repeat the same reporting of performance using the confusion matrix:\n",
        "- Did this pre-trained network do better overall?\n",
        "- In which class it improve the accuracy from the above model?\n",
        "- Which class still has low performance?\n",
        "\n",
        "Typically, your network must have a reasonable performance of at least 84% overall accuracy to be considered successful in this domain. If your network achieves a accuracy of 94% or above on the validation set, you will also recieve a 10 bonus points, so keep trying!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rt7HV46lMvJ"
      },
      "outputs": [],
      "source": [
        "y_pred = franken_model.predict(validation_ds)\n",
        "\n",
        "lst = []\n",
        "for i in range(len(y_pred)):\n",
        "  y_pred[i] = np.argmax(y_pred[i])\n",
        "  lst.append(y_pred[i][0])\n",
        "\n",
        "pred_labels = np.asarray(lst)\n",
        "\n",
        "print(type(pred_labels))\n",
        "print(type(val_labels))\n",
        "print(pred_labels[0:5], \"\\n\\n\", val_labels[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEIKidPBk1mX"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = sklearn.metrics.confusion_matrix(pred_labels, val_labels)\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUgCkBQplhah"
      },
      "outputs": [],
      "source": [
        "con_mat_df = pd.DataFrame(confusion_matrix,\n",
        "                     index = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
        "                     columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17])\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report Model Configuration\n",
        "**Num layers:** 179\n",
        "\n",
        "**Justification for Nodes per layer:** I kept the orginial ResNet50 ~175 layers the same only adding an averaging layer, then a 1 hidden layers of size 1024 dense nodes. Then a tuned dropout layer to prevent overfitting is used for class prediction in the subsequent softmax layer. No reason to pick 1024 other than I think it is suffiecently large for learning.\n",
        "\n",
        "**Activation Functions:** ReLu becuase it is the fastest activation function for training models (~6x faster than sigmoid and tanh)\n",
        "\n",
        "**Total Number of Free Parameters:**\n",
        "\n",
        "Total params: 25,704,338\n",
        "\n",
        "Trainable params: 25,651,218\n",
        "\n",
        "Non-trainable params: 53,120\n",
        "\n",
        "## Analysis of Model\n",
        "How well is my architecture is doing (overall accuracy): 0.8246 Validation Loss\n",
        "\n",
        "Where did my model makes mistake most? Model had a really hard time deserning between the labels 11,13,14,15 as idenfied in the concolution matrix by count of inaacurate predictions per categories.\n",
        "\n",
        "Which classes can be improved? All could definitely be improved, I fear my validation set is skewed to have too many images from class 17 imprlying. So then the model could blent predict label 17 more times and be \"correct\" against the validation set"
      ],
      "metadata": {
        "id": "mzEtxRk2uoEY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPmNtIGVl-7F"
      },
      "source": [
        "# Step 5: Reflection\n",
        "\n",
        "Write at least a paragraph answering these prompts: How did your own network perform in comparison to the pre-trained one? What are the major differences between the architectures? Additionally, report on your experience implementing different models for this assignment (Was it hard/easy/fun?, from which part did you learn the most?)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YtdjSICw66_"
      },
      "source": [
        "My Network preformed worse than the pre-trained one. My Model used many filters, stacked convolution layers whereas the pre-trained one realied on layer depth (~180 layers) to learn. I found this assignment very challenging. There does not seem to be any reason or rational for what layers to use, how to arange them and type hyper parameters like nodes, filters, kernels and strides. Could have been nice to see examples of grid search or something to tune these models. I guess the problem is that Neural Nets by nature are \"Black-Box\" so it is hard for me to infer what should be changed when looking at the output becuase it is so damn sensitive and seemingly irrational. I want to take an online course on deep learning network archetecture to figure some of this stuff out becuase I think it is cool. I just don't really know a good way to do it and I do not want to go to Grad School to find out.\n",
        "\n",
        "**Note for the Grader**\n",
        "please consider my pizzia post [here](https://piazza.com/class/l6yw646y6u617h/post/279) as excused justification for why this assignment was turned in late.\n",
        "I kept trying to come up with my own archecture for step 3 but nothing ever got more than 0.35 accuracy [This is why I just implemented to AlexNet Archetecture, I want to be transparent that I am not claiming that design as my orginial work for plagerism purposes]. The amount of testing I was doing ate up my GPU credits for the last day and becuase the 'franken_model' has so many layers it completely eleminated any GPU credits. I have been locked out of the GPU from 10:30pm 11/30 til at least 5:00pm 12/1. I was temporarily out of credits on my personal google account as well but this blackout was shorter"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}